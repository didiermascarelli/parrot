# parrot
The best README we thought about was to ask #chatGPT to write something about our experimentation.

[HUMAN]: we have tried #chatGPT on basic logical skills to be sure the fundamentals are met by the AI. So we took a very simple exemple with socks. Unfortunately the basics of logic are not there, therefore #chatGPT cannot be considered as a source of #truth.
[CHAT-GPT]: It appears that you have attempted to use ChatGPT for a task related to basic logical skills, using a simple example involving socks. Based on this experience, it seems that the model does not possess a strong foundation in logic, and therefore may not be an entirely reliable source of truth. It is important to note that ChatGPT is a language model, and while it has been trained on a wide range of text, it is not specifically designed for tasks involving logical reasoning.

[HUMAN]: So for what #chatGPT has been designed then?
[CHAT-GPT]: ChatGPT is a language model that was designed to generate human-like text. It is trained on a diverse range of internet text, allowing it to generate a wide variety of responses to prompts given to it. Some common uses for ChatGPT include:
Language translation
Text summarization
Text completion
Text generation
Dialogue systems and chatbots
Language modeling for natural language processing tasks
It is important to note that ChatGPT is a machine learning model, it's not designed for a specific task, but it is designed to be flexible and adaptable to a wide range of natural language processing tasks.

[HUMAN]: if #chatGPT has not been designed for tasks involving logical reasoning how can we be sure the output given by #chatGPT is logical ? 
[CHAT-GPT]: ChatGPT is a language model that has been trained on a wide range of text, and it is capable of generating coherent and plausible responses to prompts. However, it is not specifically designed for tasks involving logical reasoning, and therefore, it may not always produce output that is logically sound.
It's important to evaluate the output of ChatGPT, or any other language model, critically and use it in combination with other methods or sources of information to make sure the output is logical and accurate.
Also, it's possible to train ChatGPT on a specific task or domain to improve its performance and make it more reliable, but it requires a good understanding of the domain, a good dataset and the right approach.

[HUMAN]: so does it mean you do not comply with Gödel's incompleteness theorems
[CHAT-GPT]: Gödel's incompleteness theorems state that any formal system that is powerful enough to express basic arithmetic cannot be both consistent and complete.
As a language model, ChatGPT is not a formal system, it is a machine learning model that was trained on a large dataset of text, it is not intended to be a formal system and it's not designed to comply with Gödel's incompleteness theorems.
It is important to keep in mind that ChatGPT is a tool, and like any tool, it can be used to generate text, but it is not a substitute for human reasoning and judgement.
It's also important to note that Gödel's incompleteness theorems are a fundamental result in theoretical computer science and mathematics, and it's not related to the capabilities of a language model such as ChatGPT.
